{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Supported by TingLin**\n",
    "   - **Kansas State University**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "  - **Extracting, transforming and selecting features**\n",
    "    \n",
    "- **Feature Extractors**\n",
    "     - [TF-IDF](#TF-IDF)\n",
    "     - [Word2Vec](#Word2Vec)\n",
    "     - [CountVectorizer](#CountVectorizer)\n",
    "- **Feature Transformers**\n",
    "     - [Tokenizer](#Tokenizer)\n",
    "     - [StopWordsRemover](#StopWordsRemover)\n",
    "     - [nn-gram](#nn-gram)\n",
    "     - [Binarizer](#Binarizer)\n",
    "     - [PCA](#PCA)\n",
    "     - [PolynomialExpansion](#PolynomialExpansion)\n",
    "     - [Discrete Cosine Transform (DCT)](#Discrete Cosine Transform)\n",
    "     - [StringIndexer](#StringIndexer)\n",
    "     - [IndexToString](#IndexToString)\n",
    "     - [OneHotEncoder](#OneHotEncoder)\n",
    "     - [VectorIndexer](#VectorIndexer)\n",
    "     - [Interaction](#Interaction)\n",
    "     - [Normalizer](#Normalizer)\n",
    "     - [StandardScaler](#StandardScaler)\n",
    "     - [MinMaxScaler](#MinMaxScaler)\n",
    "     - [MaxAbsScaler](#MaxAbsScaler)\n",
    "     - [Bucketizer](#Bucketizer)\n",
    "     - [ElementwiseProduct](#ElementwiseProduct)\n",
    "     - [SQLTransformer](#SQLTransformer)\n",
    "     - [VectorAssembler](#VectorAssembler)\n",
    "     - [QuantileDiscretizer](#QuantileDiscretizer)\n",
    "- **Feature Selectors**\n",
    "     - [VectorSlicer](#VectorSlicer)\n",
    "     - [RFormula](#RFormula)\n",
    "     - [ChiSqSelector](#ChiSqSelector)\n",
    "- **Clustering**\n",
    "     - [LDA](#LDA)\n",
    "- **LDA Topic Modeling with csv file**\n",
    "     - [LDA Topic Modeling with csv file](#LDA Topic Modeling with csv file)\n",
    "- **Visualization**\n",
    "     - [Visualization](#Visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What am I going to learn from this Pyspark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apache Spark comes with an interactive shell for python as it does for Scala. The shell for python is known as “PySpark”. To use PySpark you will have to have python installed on your machine. As we know that each Linux machine comes preinstalled with python so you need not worry about python installation. To get started in a standalone mode you can download the pre-built version of spark from its official home page listed in the pre-requisites section of the PySpark tutorial.\n",
    "- Decompress the downloaded file. On decompressing the spark downloadable, you will see the following structure:\n",
    "    - bin: Holds all the binaries\n",
    "    - conf: Holds all the necessary configuration files to run any spark application\n",
    "    - ec2: Holds the scripts to launch a cluster on amazon cloud space with multiple ec2 instances\n",
    "    - lib: Holds the prebuilt libraries which make up the spark APIS.\n",
    "- Licenses:\n",
    "    - python\n",
    "    - python API\n",
    "- README.md:\n",
    "    - Holds important instructions to get started with spark\n",
    "    - Holds important startup scripts that are required to setup distributed clustter\n",
    "- CHANGES.txt:\n",
    "    - Holds all the changes information for each version of apache spark\n",
    "- data:\n",
    "    - Holds data that is used in the examples\n",
    "- examples:\n",
    "    - Has examples which are a good place to learn the usage of spark functions\n",
    "- LICENSE NOTICE:\n",
    "    - Important information\n",
    "    - R:\n",
    "    - Holds API of R language\n",
    "- RELEASE:\n",
    "    - Holds make info of the downloaded version\n",
    "\n",
    "- Hadoop enables distributed, low-cost storage for this growing amount of unstructured data. In this post, I'll show one way to analyze unstructured data using Apache Spark. Spark is advantageous for text analytics because it provides a platform for scalable, distributed computing.\n",
    "\n",
    "- When it comes to text analytics, you have a few option for analyzing text. I like to categorize these techniques like this:\n",
    "\n",
    "    - Text Mining (i.e. Text clustering, data-driven topics)\n",
    "    - Categorization (i.e. Tagging unstructured data into categories and sub-categories; hierarchies; taxonomies)\n",
    "    - Entity Extraction (i.e. Extracting patterns such as phrases, addresses, product codes, phone numbers, etc.)\n",
    "    - Sentiment Analysis (i.e. Tagging positive, negative, or neutral with varying levels of sentiment)\n",
    "    - Deep Linguistics (i.e Semantics. Understanding causality, purpose, time, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Extracting, transforming and selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.1 Feature Extractors**\n",
    "<a id = 'Feature Extractors'></a>\n",
    "   - ** 2.1.1 TF-IDF**\n",
    "    <a id = 'TF-IDF'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = sqlContext.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by tt, a document by dd, and the corpus by DD. Term frequency TF(t,d)TF(t,d) is the number of times that term tt appears in document dd, while document frequency DF(t,D)DF(t,D) is the number of documents that contains term tt. If we only use term frequency to measure the importance, it is very easy to over-emphasize terms that appear very often but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides:\n",
    "    - IDF(t,D)=log|D|+1DF(t,D)+1,\n",
    "    - IDF(t,D)=log⁡|D|+1DF(t,D)+1,\n",
    "- where |D|is the total number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
    "    - TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n",
    "    - TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n",
    "- There are several variants on the definition of term frequency and document frequency. In MLlib, we separate TF and IDF to make them flexible.\n",
    "\n",
    "- TF: Both HashingTF and CountVectorizer can be used to generate the term frequency vectors.\n",
    "\n",
    "    - HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a “set of terms” might be a bag of words. HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3. Then term frequencies are calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing. To reduce the chance of collision, we can increase the target feature dimension, i.e. the number of buckets of the hash table. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the feature dimension, otherwise the features will not be mapped evenly to the columns. The default feature dimension is 218=262,144218=262,144. An optional binary toggle parameter controls term frequency counts. When set to true all nonzero frequency counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts.\n",
    "\n",
    "    - CountVectorizer converts text documents to vectors of term counts. Refer to CountVectorizer for more details.\n",
    "\n",
    "IDF: IDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "   - ** 2.1.2 Word2Vec**\n",
    "    <a id = 'Word2Vec'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.00754214553162,-0.0373112341389,0.0177642568946]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.0172512845269,0.0307334170876,0.0469989763972]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.101050335169,-0.0430820055306,0.00582689233124]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = sqlContext.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document; this vector can then be used as features for prediction, document similarity calculations, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ** 2.1.3 CountVectorizer**\n",
    "    <a id = 'CountVectorizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
    "\n",
    "- During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2 Feature Transformers**\n",
    "<a id = 'Feature Transformers'></a>\n",
    "   - ** 2.2.1 Tokenizer**\n",
    "    <a id = 'Tokenizer'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). A simple Tokenizer class provides this functionality. The example below shows how to split sentences into sequences of words.\n",
    "\n",
    "- RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **2.2.2 Stopwordsremover**\n",
    "    <a id = 'Stopwordsremover'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = sqlContext.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter. Default stop words for some languages are accessible by calling StopWordsRemover.loadDefaultStopWords(language), for which available options are “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”. A boolean parameter caseSensitive indicates if the matches should be case sensitive (false by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2.3  nn-gram**\n",
    "<a id = 'nn-gram'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = sqlContext.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n",
    "\n",
    "- NGram takes as input a sequence of strings (e.g. the output of a Tokenizer). The parameter n is used to determine the number of terms in each nn-gram. The output will consist of a sequence of nn-grams where each nn-gram is represented by a space-delimited string of n consecutive words. If the input sequence contains fewer than n strings, no output is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.4 Binarizer**\n",
    "<a id = 'Binarizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = sqlContext.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binarization is the process of thresholding numerical features to binary (0/1) features.\n",
    "\n",
    "- Binarizer takes the common parameters inputCol and outputCol, as well as the threshold for binarization. Feature values greater than the threshold are binarized to 1.0; values equal to or less than the threshold are binarized to 0.0. Both Vector and Double types are supported for inputCol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.5 PCA**\n",
    "<a id = 'PCA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = sqlContext.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A PCA class trains a model to project vectors to a low-dimensional space using PCA. The example below shows how to project 5-dimensional feature vectors into 3-dimensional principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.6 PolynomialExpansion**\n",
    "<a id = 'PolynomialExpansion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+\n",
      "|features  |polyFeatures                              |\n",
      "+----------+------------------------------------------+\n",
      "|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n",
      "|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n",
      "|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n",
      "+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),),\n",
    "    (Vectors.dense([3.0, -1.0]),)\n",
    "], [\"features\"])\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)\n",
    "\n",
    "polyDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial expansion is the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original dimensions. A PolynomialExpansion class provides this functionality. The example below shows how to expand your features into a 3-degree polynomial space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2.7 Discrete Cosine Transform (DCT)**\n",
    "<a id = 'Discrete Cosine Transform'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|featuresDCT                                                     |\n",
      "+----------------------------------------------------------------+\n",
      "|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|\n",
      "|[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |\n",
      "|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n",
    "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n",
    "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
    "\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
    "\n",
    "dctDf = dct.transform(df)\n",
    "\n",
    "dctDf.select(\"featuresDCT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Discrete Cosine Transform transforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain. A DCT class provides this functionality, implementing the DCT-II and scaling the result by 1/2‾√1/2 such that the representing matrix for the transform is unitary. No shift is applied to the transformed sequence (e.g. the 00th element of the transformed sequence is the 00th DCT coefficient and not the N/2N/2th).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.8 StringIndexer**\n",
    "<a id = 'StringIndexer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. The unseen labels will be put at index numLabels if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.9 IndexToString**\n",
    "<a id = 'IndexToString'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings. A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString. However, you are free to supply your own labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.10 OneHotEncoder**\n",
    "<a id = 'OneHotEncoder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.11 VectorIndexer**\n",
    "<a id = 'VectorIndexer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose 351 categorical features: 645, 69, 365, 138, 101, 479, 333, 249, 0, 555, 666, 88, 170, 115, 276, 308, 5, 449, 120, 247, 614, 677, 202, 10, 56, 533, 142, 500, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 480, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 583, 285, 224, 650, 615, 9, 53, 169, 593, 141, 610, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 637, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 508, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 161, 279, 64, 689, 17, 149, 584, 562, 176, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 313, 611, 144, 49, 335, 86, 672, 172, 113, 681, 219, 419, 81, 230, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 162, 531, 680, 255, 648, 112, 617, 194, 145, 48, 557, 690, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 175, 446, 612, 143, 43, 250, 231, 450, 99, 363, 556, 87, 203, 671, 688, 104, 368, 588, 40, 304, 26, 258, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 536, 82, 620, 447, 36, 168, 146, 30, 51, 190, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 664, 532, 94, 283, 395, 332, 528, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 122, 83, 309, 560, 639, 676, 222, 592, 364, 100\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|             indexed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "data = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(data)\n",
    "\n",
    "categoricalFeatures = indexerModel.categoryMaps\n",
    "print(\"Chose %d categorical features: %s\" %\n",
    "      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(data)\n",
    "indexedData.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VectorIndexer helps index categorical features in datasets of Vectors. It can both automatically decide which features are categorical and convert original values to category indices. Specifically, it does the following:\n",
    "\n",
    "- Take an input column of type Vector and a parameter maxCategories.\n",
    "- Decide which features should be categorical based on the number of distinct values, where features with at most maxCategories are declared categorical.\n",
    "- Compute 0-based category indices for each categorical feature.\n",
    "- Index categorical features and transform original feature values to indices.\n",
    "- Indexing categorical features allows algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.12 Normalizer**\n",
    "<a id = 'Normalizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n",
      "Normalized using L^inf norm\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.13 StandardScaler**\n",
    "<a id = 'StandardScaler'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "    - withStd: True by default. Scales the data to unit standard deviation.\n",
    "    - withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.\n",
    "- StandardScaler is an Estimator which can be fit on a dataset to produce a StandardScalerModel; this amounts to computing summary statistics. The model can then transform a Vector column in a dataset to have unit standard deviation and/or zero mean features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.14 MinMaxScaler**\n",
    "<a id = 'MinMaxScaler'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+--------------+--------------+\n",
      "|      features|scaledFeatures|\n",
      "+--------------+--------------+\n",
      "|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|\n",
      "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MinMaxScaler transforms a dataset of Vector rows, rescaling each feature to a specific range (often [0, 1]). It takes parameters:\n",
    "\n",
    "    - min: 0.0 by default. Lower bound after transformation, shared by all features.\n",
    "    - max: 1.0 by default. Upper bound after transformation, shared by all features.\n",
    "- MinMaxScaler computes summary statistics on a data set and produces a MinMaxScalerModel. The model can then transform each feature individually such that it is in the given range.\n",
    "\n",
    "- The rescaled value for a feature E is calculated as,\n",
    "       - Rescaled(ei)=(ei−Emin)/(Emax−Emin)∗(max−min)+min\n",
    "- For the case Emax==Emin, Rescaled(ei)=0.5∗(max+min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.15 MaxAbsScaler**\n",
    "<a id = 'MaxAbsScaler'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      features|  scaledFeatures|\n",
      "+--------------+----------------+\n",
      "|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]|\n",
      "|[2.0,1.0,-4.0]|  [0.5,0.1,-0.5]|\n",
      "|[4.0,10.0,8.0]|   [1.0,1.0,1.0]|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MaxAbsScaler transforms a dataset of Vector rows, rescaling each feature to range [-1, 1] by dividing through the maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "\n",
    "- MaxAbsScaler computes summary statistics on a data set and produces a MaxAbsScalerModel. The model can then transform each feature individually to range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2.16 Bucketizer**\n",
    "<a id = 'Bucketizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = sqlContext.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter:\n",
    "\n",
    "    - splits: Parameter for mapping continuous features into buckets. With n+1 splits, there are n buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last bucket, which also includes y. Splits should be strictly increasing. Values at -inf, inf must be explicitly provided to cover all Double values; Otherwise, values outside the splits specified will be treated as errors. Two examples of splits are Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity) and Array(0.0, 1.0, 2.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.17 ElementwiseProduct**\n",
    "<a id = 'ElementwiseProduct'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|       vector|transformedVector|\n",
      "+-------------+-----------------+\n",
      "|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create some vector data; also works for sparse vectors\n",
    "data = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]\n",
    "df = sqlContext.createDataFrame(data, [\"vector\"])\n",
    "transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),\n",
    "                                 inputCol=\"vector\", outputCol=\"transformedVector\")\n",
    "# Batch transform the vectors to create new column:\n",
    "transformer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ElementwiseProduct multiplies each input vector by a provided “weight” vector, using element-wise multiplication. In other words, it scales each column of the dataset by a scalar multiplier. This represents the Hadamard product between the input vector, v and transforming vector, w, to yield a result vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.18 SQLTransformer**\n",
    "<a id = 'SQLTransformer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, 1.0, 3.0),\n",
    "    (2, 2.0, 5.0)\n",
    "], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "sqlTrans.transform(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QLTransformer implements the transformations which are defined by SQL statement. Currently we only support SQL syntax like \"SELECT ... FROM __THIS__ ...\" where \"__THIS__\" represents the underlying table of the input dataset. The select clause specifies the fields, constants, and expressions to display in the output, and can be any select clause that Spark SQL supports. Users can also use Spark SQL built-in function and UDFs to operate on these selected columns. For example, SQLTransformer supports statements like:\n",
    "\n",
    "- SELECT a, a + b AS a_b FROM __THIS__\n",
    "- SELECT a, SQRT(b) AS b_sqrt FROM __THIS__ where a > 5\n",
    "- SELECT a, b, SUM(c) AS c_sum FROM __THIS__ GROUP BY a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.2.19 VectorAssembler**\n",
    "<a id = 'VectorAssembler'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = sqlContext.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.3 Feature Selectors**\n",
    "<a id = 'Feature Selectors'></a>\n",
    "    - **2.3.1 VectorSlicer**\n",
    "    <a id = 'VectorSlicer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|        userFeatures|     features|\n",
      "+--------------------+-------------+\n",
      "|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|\n",
      "|      [-2.0,2.3,0.0]|        [2.3]|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]))])\n",
    "\n",
    "slicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[1])\n",
    "\n",
    "output = slicer.transform(df)\n",
    "\n",
    "output.select(\"userFeatures\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VectorSlicer is a transformer that takes a feature vector and outputs a new feature vector with a sub-array of the original features. It is useful for extracting features from a vector column.\n",
    "\n",
    "- VectorSlicer accepts a vector column with specified indices, then outputs a new vector column whose values are selected via those indices. There are two types of indices,\n",
    "\n",
    "- Integer indices that represent the indices into the vector, setIndices().\n",
    "\n",
    "- String indices that represent the names of features into the vector, setNames(). This requires the vector column to have an AttributeGroup since the implementation matches on the name field of an Attribute.\n",
    "\n",
    "- Specification by integer and string are both acceptable. Moreover, you can use integer index and string name simultaneously. At least one feature must be selected. Duplicate features are not allowed, so there can be no overlap between selected indices and names. Note that if names of features are selected, an exception will be thrown if empty input attributes are encountered.\n",
    "\n",
    "- The output vector will order features with the selected indices first (in the order given), followed by the selected names (in the order given)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.3.2 RFormula** \n",
    "<a id = 'RFormula'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[0.0,0.0,18.0]|  1.0|\n",
      "|[0.0,1.0,12.0]|  0.0|\n",
      "|[1.0,0.0,15.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "dataset = sqlContext.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RFormula selects columns specified by an R model formula. Currently we support a limited subset of the R operators, including ‘~’, ‘.’, ‘:’, ‘+’, and ‘-‘. The basic operators are:\n",
    "\n",
    "- ~ separate target and terms\n",
    "- + concat terms, “+ 0” means removing intercept\n",
    "- - remove a term, “- 1” means removing intercept\n",
    "- : interaction (multiplication for numeric values, or binarized categorical values)\n",
    "- . all columns except target\n",
    "- Suppose a and b are double columns, we use the following simple examples to illustrate the effect of RFormula:\n",
    "\n",
    "- y ~ a + b means model y ~ w0 + w1 * a + w2 * b where w0 is the intercept and w1, w2 are coefficients.\n",
    "- y ~ a + b + a:b - 1 means model y ~ w1 * a + w2 * b + w3 * a * b where w1, w2, w3 are coefficients.\n",
    "- RFormula produces a vector column of features and a double or string column of label. Like when formulas are used in R for linear regression, string input columns will be one-hot encoded, and numeric columns will be cast to doubles. If the label column is of type string, it will be first transformed to double with StringIndexer. If the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** 2.3.3 ChiSqSelector**\n",
    "<a id = 'ChiSqSelector'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ChiSqSelector stands for Chi-Squared feature selection. It operates on labeled data with categorical features. ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose. It supports five selection methods: numTopFeatures, percentile, fpr, fdr, fwe: * numTopFeatures chooses a fixed number of top features according to a chi-squared test. This is akin to yielding the features with the most predictive power. * percentile is similar to numTopFeatures but chooses a fraction of all features instead of a fixed number. * fpr chooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection. * fdr uses the Benjamini-Hochberg procedure to choose all features whose false discovery rate is below a threshold. * fwe chooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection. By default, the selection method is numTopFeatures, with the default number of top features set to 50. The user can choose a selection method using setSelectorType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unlable topic modeling\n",
    "<a id = 'Unlable topic modeling'></a>\n",
    "- ** 3.1 LDA Modeling**\n",
    "<a id = 'LDA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"data/airlines.csv\").zipWithIndex().map(lambda (review,uid): Row(uid= uid, words = review.split(\" \")))\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "docDF = sqlContext.createDataFrame(data) \n",
    "docDF = remover.transform(docDF)\n",
    "Vector = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "model = Vector.fit(docDF)\n",
    "result = model.transform(docDF)\n",
    "\n",
    "corpus = result.select(\"uid\", \"vectors\").rdd.map(lambda (x,y): [x,Vectors.fromML(y)]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0:\n",
      "fish\n",
      "lamb\n",
      "UAL\n",
      "flatbeds\n",
      "outlets.\n",
      "display\n",
      "recliner\n",
      "fairness\n",
      "primary\n",
      "no-fly\n",
      "\n",
      "\n",
      "Topic1:\n",
      "flight\n",
      "time\n",
      "flights\n",
      "service\n",
      "us\n",
      "-\n",
      "plane\n",
      "hours\n",
      "get\n",
      "United\n",
      "\n",
      "\n",
      "Topic2:\n",
      "global\n",
      "flight\n",
      "slipper\n",
      "boxes\n",
      "Lisbon\n",
      "alone.\n",
      "smiles.\n",
      "dead\n",
      "salad\n",
      "Minot\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3,maxIterations=100,optimizer='online')\n",
    "topics = ldaModel.topicsMatrix()\n",
    "vocabArray = model.vocabulary\n",
    "\n",
    "wordNumbers = 10  # number of words per topic\n",
    "topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))\n",
    "\n",
    "def topic_render(topic):  # specify vector id of words to actual words\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "topics_final = topicIndices.map(lambda topic: topic_render(topic)).collect()\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print (\"Topic\" + str(topic) + \":\")\n",
    "    for term in topics_final[topic]:\n",
    "        print (term)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: LDA does not perform well with the EMLDAOptimizer which is used by default. In the case of EMLDAOptimizer we have significant bies to the most popular hashtags. I used the OnlineLDAOptimizer instead. The Optimizer implements the Online variational Bayes LDA algorithm, which processes a subset of the corpus on each iteration, and updates the term-topic distribution adaptively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling with csv file\n",
    "<a id = 'LDA Topic Modeling with csv file'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
      "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|uid|year_month|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
      "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|   21-Jun-|\n",
      "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|   19-Jun-|\n",
      "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|  2|   18-Jun-|\n",
      "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|  3|   17-Jun-|\n",
      "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|   17-Jun-|\n",
      "|10006|Delta Air Lines|17-Jun-14|     USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|   17-Jun-|\n",
      "|10007|Delta Air Lines|14-Jun-14|      UK|     0| Economy|    1|         NO|Flight from NY La...|  6|   14-Jun-|\n",
      "|10008|Delta Air Lines|14-Jun-14|     USA|     0| Economy|    1|         NO|Originally I had ...|  7|   14-Jun-|\n",
      "|10009|Delta Air Lines|13-Jun-14|     USA|     4|Business|    2|         NO|We flew paid busi...|  8|   13-Jun-|\n",
      "|10010|Delta Air Lines|13-Jun-14|      UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|   13-Jun-|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "rawdata = sqlContext.read.load(\"data/airlines2.csv\", format=\"csv\", header=True)\n",
    "rawdata = rawdata.fillna({'review': ''})                               # Replace nulls with blank string\n",
    "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())     # Create Unique ID\n",
    "rawdata = rawdata.withColumn(\"year_month\", rawdata.date.substr(1,7))   # Generate YYYY-MM variable\n",
    " \n",
    "# Show rawdata (as DataFrame)\n",
    "rawdata.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creat new column as unique id, which is use for vectorizing words with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 'string')\n",
      "('airline', 'string')\n",
      "('date', 'string')\n",
      "('location', 'string')\n",
      "('rating', 'string')\n",
      "('cabin', 'string')\n",
      "('value', 'string')\n",
      "('recommended', 'string')\n",
      "('review', 'string')\n",
      "('uid', 'bigint')\n",
      "('year_month', 'string')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rating', 'int')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print data types\n",
    "for type in rawdata.dtypes:\n",
    "    print type\n",
    "\n",
    "target = rawdata.select(rawdata['rating'].cast(IntegerType()))\n",
    "target.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_text(record):\n",
    "    text  = record[8]\n",
    "    uid   = record[9]\n",
    "    words = text.split()  \n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]    \n",
    "    \n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n",
    "    return text_out\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-create words column as string based on rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'10001', airline=u'Delta Air Lines', date=u'21-Jun-14', location=u'Thailand', rating=u'7', cabin=u'Economy', value=u'4', recommended=u'YES', review=u'Flew Mar 30 NRT to BKK. All flights were great. Flight was on-time and the in-flight entertainment was great. Apart from the meals - some Thai passengers cannot eat beef so the flight crews tried to ask other passengers who could eat beef and changed the meals around. We feel disappointed with their food services.', uid=0, year_month=u'21-Jun-', words=[u'flew', u'mar', u'nrt', u'bkk', u'flights', u'great', u'flight', u'ontime', u'inflight', u'entertainment', u'great', u'apart', u'meals', u'thai', u'passengers', u'cannot', u'eat', u'beef', u'flight', u'crews', u'tried', u'ask', u'passengers', u'eat', u'beef', u'changed', u'meals', u'around', u'feel', u'disappointed', u'food', u'services'])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first row of new dataset\n",
    "clean_text.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "# Vectorize words arrays for each uid\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "# Find TF-IDF coefficients for each word instead of bag of words\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'10001', airline=u'Delta Air Lines', date=u'21-Jun-14', location=u'Thailand', rating=u'7', cabin=u'Economy', value=u'4', recommended=u'YES', review=u'Flew Mar 30 NRT to BKK. All flights were great. Flight was on-time and the in-flight entertainment was great. Apart from the meals - some Thai passengers cannot eat beef so the flight crews tried to ask other passengers who could eat beef and changed the meals around. We feel disappointed with their food services.', uid=0, year_month=u'21-Jun-', words=[u'flew', u'mar', u'nrt', u'bkk', u'flights', u'great', u'flight', u'ontime', u'inflight', u'entertainment', u'great', u'apart', u'meals', u'thai', u'passengers', u'cannot', u'eat', u'beef', u'flight', u'crews', u'tried', u'ask', u'passengers', u'eat', u'beef', u'changed', u'meals', u'around', u'feel', u'disappointed', u'food', u'services'], rawFeatures=SparseVector(1000, {0: 2.0, 3: 1.0, 11: 1.0, 25: 1.0, 32: 2.0, 46: 1.0, 56: 2.0, 97: 1.0, 113: 1.0, 201: 1.0, 213: 1.0, 249: 2.0, 332: 1.0, 346: 1.0, 369: 1.0, 395: 1.0, 490: 1.0, 509: 1.0, 537: 1.0, 621: 2.0, 693: 1.0, 846: 2.0}), features=SparseVector(1000, {0: 0.4099, 3: 1.0601, 11: 1.2624, 25: 1.3913, 32: 3.4155, 46: 1.8131, 56: 4.3116, 97: 2.3469, 113: 2.5063, 201: 2.8577, 213: 2.9304, 249: 6.1442, 332: 3.3172, 346: 3.3454, 369: 3.435, 395: 3.4667, 490: 3.7227, 509: 3.8097, 537: 3.6819, 621: 8.2563, 693: 4.1281, 846: 8.6716}))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first row of final dataset\n",
    "rescaledData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add features as new column based on clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[106, 301, 432, 7...|[0.02642483914460...|\n",
      "|    1|[218, 257, 312, 4...|[0.02860477465189...|\n",
      "|    2|[869, 639, 155, 2...|[0.01437555629232...|\n",
      "|    3|[139, 155, 50, 12...|[0.02946689912509...|\n",
      "|    4|[582, 640, 33, 16...|[0.01636880051698...|\n",
      "|    5|[498, 251, 48, 26...|[0.01998824201487...|\n",
      "|    6|[197, 791, 88, 39...|[0.03639006364794...|\n",
      "|    7|[459, 248, 386, 1...|[0.01892117257323...|\n",
      "|    8|[237, 761, 78, 31...|[0.01975997181346...|\n",
      "|    9|[411, 8, 629, 47,...|[0.01284467745242...|\n",
      "|   10|[573, 796, 723, 1...|[0.01320751612923...|\n",
      "|   11|[500, 363, 392, 2...|[0.02222368774827...|\n",
      "|   12|[136, 182, 5, 22,...|[0.01628267590187...|\n",
      "|   13|[780, 327, 368, 7...|[0.01180130842975...|\n",
      "|   14|[19, 71, 29, 4, 3...|[0.01849898845552...|\n",
      "|   15|[54, 8, 26, 389, ...|[0.02528842185304...|\n",
      "|   16|[601, 435, 100, 6...|[0.01486335804649...|\n",
      "|   17|[803, 750, 942, 4...|[0.01321685197525...|\n",
      "|   18|[607, 830, 156, 8...|[0.01302755207792...|\n",
      "|   19|[431, 980, 536, 7...|[0.01075254461350...|\n",
      "|   20|[21, 254, 158, 11...|[0.01763835986304...|\n",
      "|   21|[406, 274, 584, 1...|[0.02274192838402...|\n",
      "|   22|[84, 122, 86, 443...|[0.02436179391391...|\n",
      "|   23|[37, 786, 161, 90...|[0.01646874552174...|\n",
      "|   24|[410, 442, 548, 7...|[0.01560515003952...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------------------------------------+\n",
      "|topic|topic_desc                                                                                   |\n",
      "+-----+---------------------------------------------------------------------------------------------+\n",
      "|0    |[extra, row, main, segments, legroom, exit, cabin, pay, seat, seats]                         |\n",
      "|1    |[vegas, las, upgrade, economy, upgraded, seat, online, internal, dec, seats]                 |\n",
      "|2    |[oakland, nearly, phoenix, less, united, york, couple, lax, time, flights]                   |\n",
      "|3    |[san, phoenix, day, missed, connection, told, desk, connecting, diego, leaving]              |\n",
      "|4    |[son, supervisor, boarding, different, told, seattle, agent, southwest, got, check]          |\n",
      "|5    |[clt, wife, airways, class, miles, lga, club, coach, first, seats]                           |\n",
      "|6    |[phl, louisville, philadelphia, washington, laguardia, york, set, airways, rude, houston]    |\n",
      "|7    |[carry, overhead, recline, seat, fit, ice, may, crew, drink, price]                          |\n",
      "|8    |[sfo, dtw, southwest, orlando, lhr, delta, trip, flights, seats, storm]                      |\n",
      "|9    |[min, delta, representative, check, bus, landing, anyone, terminal, baggage, got]            |\n",
      "|10   |[lady, proceeded, wheelchair, ticket, name, cleveland, columbus, luggage, lost, ewr]         |\n",
      "|11   |[paris, detroit, philly, hrs, united, wifi, flights, hour, family, plane]                    |\n",
      "|12   |[denver, mechanical, hours, delayed, airport, connecting, hour, night, next, stuck]          |\n",
      "|13   |[priority, average, quality, guess, amsterdam, poor, nothing, etc, low, flights]             |\n",
      "|14   |[gate, bags, minutes, plane, passes, milwaukee, told, charlotte, sat, boarding]              |\n",
      "|15   |[business, delta, class, deltas, lax, 777, meal, cabin, sydney, usa]                         |\n",
      "|16   |[tampa, ord, atlanta, canada, day, got, cancelled, 2014, florida, next]                      |\n",
      "|17   |[phx, honolulu, hnl, economy, configuration, american, premium, 300, entertainment, class]   |\n",
      "|18   |[today, text, scheduled, frankfurt, actually, received, depart, gate, airport, philadelphia] |\n",
      "|19   |[dublin, louis, absolutely, france, usually, always, airplane, beginning, baggage, delays]   |\n",
      "|20   |[good, selection, served, excellent, drinks, better, comfortable, friendly, efficient, great]|\n",
      "|21   |[dallas, london, toronto, chicago, cancelled, rep, connecting, ohare, atlanta, next]         |\n",
      "|22   |[line, agent, asked, beverage, called, cup, agents, request, customer, boston]               |\n",
      "|23   |[return, mexico, seating, nov, ridiculous, delta, flew, knew, since, show]                   |\n",
      "|24   |[despite, employees, envoy, confirmed, level, lounge, phl, outbound, 1st, boarding]          |\n",
      "+-----+---------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|        airline|     date| location|rating|   cabin|value|recommended|              review|uid|year_month|               words|         rawFeatures|            features|   topicDistribution|\n",
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|10001|Delta Air Lines|21-Jun-14| Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|   21-Jun-|[flew, mar, nrt, ...|(1000,[0,3,11,25,...|(1000,[0,3,11,25,...|[0.03326472639955...|\n",
      "|10002|Delta Air Lines|19-Jun-14|      USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|   19-Jun-|[flight, 2463, le...|(1000,[0,1,5,8,15...|(1000,[0,1,5,8,15...|[0.03232387302675...|\n",
      "|10003|Delta Air Lines|18-Jun-14|      USA|     0| Economy|    1|         NO|Delta Website fro...|  2|   18-Jun-|[delta, website, ...|(1000,[0,3,4,8,16...|(1000,[0,3,4,8,16...|[0.02085701183572...|\n",
      "|10004|Delta Air Lines|17-Jun-14|      USA|     9|Business|    4|        YES|\"I just returned ...|  3|   17-Jun-|[returned, roundt...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[0.04526978916637...|\n",
      "|10005|Delta Air Lines|17-Jun-14|  Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|   17-Jun-|[roundtrip, fligh...|(1000,[0,4,8,10,1...|(1000,[0,4,8,10,1...|[0.02298136330948...|\n",
      "|10006|Delta Air Lines|17-Jun-14|      USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|   17-Jun-|[narita, bangkok,...|(1000,[0,2,3,9,11...|(1000,[0,2,3,9,11...|[0.06414106247821...|\n",
      "|10007|Delta Air Lines|14-Jun-14|       UK|     0| Economy|    1|         NO|Flight from NY La...|  6|   14-Jun-|[flight, guardia,...|(1000,[0,5,8,15,1...|(1000,[0,5,8,15,1...|[0.03727856238573...|\n",
      "|10008|Delta Air Lines|14-Jun-14|      USA|     0| Economy|    1|         NO|Originally I had ...|  7|   14-Jun-|[originally, hour...|(1000,[3,4,8,15,1...|(1000,[3,4,8,15,1...|[0.05743582089384...|\n",
      "|10009|Delta Air Lines|13-Jun-14|      USA|     4|Business|    2|         NO|We flew paid busi...|  8|   13-Jun-|[flew, paid, busi...|(1000,[0,1,2,4,7,...|(1000,[0,1,2,4,7,...|[0.00818601927398...|\n",
      "|10010|Delta Air Lines|13-Jun-14|       UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|   13-Jun-|[flew, heathrow, ...|(1000,[0,2,8,10,1...|(1000,[0,2,8,10,1...|[0.02731298418439...|\n",
      "|10011|Delta Air Lines|11-Jun-14|      USA|    10| Economy|    4|        YES|I was a bit stubb...| 10|   11-Jun-|[bit, stubborn, f...|(1000,[0,1,8,9,14...|(1000,[0,1,8,9,14...|[0.03436260105384...|\n",
      "|10012|Delta Air Lines|10-Jun-14|Australia|    10| Economy|    5|        YES|JFK-LHR. Had a gr...| 11|   10-Jun-|[jfklhr, great, t...|(1000,[0,1,2,7,8,...|(1000,[0,1,2,7,8,...|[0.03453878652317...|\n",
      "|10013|Delta Air Lines| 9-Jun-14|      USA|     0| Economy|    1|         NO|My wife and I fly...| 12|   9-Jun-1|[wife, fly, frequ...|(1000,[0,8,12,15,...|(1000,[0,8,12,15,...|[0.01963690229259...|\n",
      "|10014|Delta Air Lines| 9-Jun-14|      USA|    10| Premium|    5|        YES|DL 1134 PBI-ATL. ...| 13|   9-Jun-1|[1134, pbiatl, gr...|(1000,[0,1,3,4,8,...|(1000,[0,1,3,4,8,...|[0.04123527369840...|\n",
      "|10015|Delta Air Lines| 6-Jun-14|      USA|     0| Economy|    2|         NO|Our flight from F...| 14|   6-Jun-1|[flight, fairbank...|(1000,[0,2,3,8,15...|(1000,[0,2,3,8,15...|[0.03666469973448...|\n",
      "|10016|Delta Air Lines| 5-Jun-14|      USA|     0| Economy|    1|         NO|On May 22 after a...| 15|   5-Jun-1|[may, 6hour, dela...|(1000,[0,1,8,12,2...|(1000,[0,1,8,12,2...|[0.01631383899797...|\n",
      "|10017|Delta Air Lines| 3-Jun-14|   Canada|     9| Economy|    4|        YES|Considering how D...| 16|   3-Jun-1|[considering, del...|(1000,[0,2,3,8,18...|(1000,[0,2,3,8,18...|[0.02790404916619...|\n",
      "|10018|Delta Air Lines| 2-Jun-14|      USA|     9| Economy|    5|        YES|Travelled MSP-LHR...| 17|   2-Jun-1|[travelled, msplh...|(1000,[0,1,3,7,8,...|(1000,[0,1,3,7,8,...|[0.08869373392228...|\n",
      "|10019|Delta Air Lines|29-May-14|      USA|     7|Business|    2|        YES|JFK-LAX on a 757-...| 18|   29-May-|[jfklax, 757200, ...|(1000,[0,2,7,9,11...|(1000,[0,2,7,9,11...|[0.02730586764691...|\n",
      "|10020|Delta Air Lines|28-May-14|       UK|     9| Economy|    3|        YES|Third long haul f...| 19|   28-May-|[third, long, hau...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[0.03560516189225...|\n",
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA Topic modeling\n",
    "lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(25,False)\n",
    "\n",
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+\n",
      "|   id|        airline|     date| location|rating|   cabin|value|recommended|              review|uid|year_month|               words|         rawFeatures|            features|   topicDistribution|   Topic_12|   topic_20|\n",
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+\n",
      "|10001|Delta Air Lines|21-Jun-14| Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|  0|   21-Jun-|[flew, mar, nrt, ...|(1000,[0,3,11,25,...|(1000,[0,3,11,25,...|[0.03326458340521...|0.025253555|  0.0619217|\n",
      "|10002|Delta Air Lines|19-Jun-14|      USA|     0| Economy|    2|         NO|Flight 2463 leavi...|  1|   19-Jun-|[flight, 2463, le...|(1000,[0,1,5,8,15...|(1000,[0,1,5,8,15...|[0.03232628208169...|0.082599744|0.020200066|\n",
      "|10003|Delta Air Lines|18-Jun-14|      USA|     0| Economy|    1|         NO|Delta Website fro...|  2|   18-Jun-|[delta, website, ...|(1000,[0,3,4,8,16...|(1000,[0,3,4,8,16...|[0.02085695505301...|0.040913098|0.017439442|\n",
      "|10004|Delta Air Lines|17-Jun-14|      USA|     9|Business|    4|        YES|\"I just returned ...|  3|   17-Jun-|[returned, roundt...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[0.04526404925230...|0.017391125| 0.12358886|\n",
      "|10005|Delta Air Lines|17-Jun-14|  Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|  4|   17-Jun-|[roundtrip, fligh...|(1000,[0,4,8,10,1...|(1000,[0,4,8,10,1...|[0.02298163017290...|0.021208351|0.034675542|\n",
      "|10006|Delta Air Lines|17-Jun-14|      USA|     9|Business|    5|        YES|Narita - Bangkok ...|  5|   17-Jun-|[narita, bangkok,...|(1000,[0,2,3,9,11...|(1000,[0,2,3,9,11...|[0.06415012692178...|0.024676414| 0.26749963|\n",
      "|10007|Delta Air Lines|14-Jun-14|       UK|     0| Economy|    1|         NO|Flight from NY La...|  6|   14-Jun-|[flight, guardia,...|(1000,[0,5,8,15,1...|(1000,[0,5,8,15,1...|[0.03727845697947...| 0.10141528|0.017945928|\n",
      "|10008|Delta Air Lines|14-Jun-14|      USA|     0| Economy|    1|         NO|Originally I had ...|  7|   14-Jun-|[originally, hour...|(1000,[3,4,8,15,1...|(1000,[3,4,8,15,1...|[0.05743576380915...| 0.03872434| 0.02677947|\n",
      "|10009|Delta Air Lines|13-Jun-14|      USA|     4|Business|    2|         NO|We flew paid busi...|  8|   13-Jun-|[flew, paid, busi...|(1000,[0,1,2,4,7,...|(1000,[0,1,2,4,7,...|[0.00818601712142...|0.009902713|0.009590511|\n",
      "|10010|Delta Air Lines|13-Jun-14|       UK|     9| Economy|    3|        YES|\"I flew from Heat...|  9|   13-Jun-|[flew, heathrow, ...|(1000,[0,2,8,10,1...|(1000,[0,2,8,10,1...|[0.02731072587980...|0.014978534| 0.47009525|\n",
      "|10011|Delta Air Lines|11-Jun-14|      USA|    10| Economy|    4|        YES|I was a bit stubb...| 10|   11-Jun-|[bit, stubborn, f...|(1000,[0,1,8,9,14...|(1000,[0,1,8,9,14...|[0.03436605090199...|0.033329412| 0.07815036|\n",
      "|10012|Delta Air Lines|10-Jun-14|Australia|    10| Economy|    5|        YES|JFK-LHR. Had a gr...| 11|   10-Jun-|[jfklhr, great, t...|(1000,[0,1,2,7,8,...|(1000,[0,1,2,7,8,...|[0.03453886536854...|0.025291754|0.031639725|\n",
      "|10013|Delta Air Lines| 9-Jun-14|      USA|     0| Economy|    1|         NO|My wife and I fly...| 12|   9-Jun-1|[wife, fly, frequ...|(1000,[0,8,12,15,...|(1000,[0,8,12,15,...|[0.01963684534294...| 0.13302182|0.016855575|\n",
      "|10014|Delta Air Lines| 9-Jun-14|      USA|    10| Premium|    5|        YES|DL 1134 PBI-ATL. ...| 13|   9-Jun-1|[1134, pbiatl, gr...|(1000,[0,1,3,4,8,...|(1000,[0,1,3,4,8,...|[0.04123384656252...|0.029665366| 0.03732407|\n",
      "|10015|Delta Air Lines| 6-Jun-14|      USA|     0| Economy|    2|         NO|Our flight from F...| 14|   6-Jun-1|[flight, fairbank...|(1000,[0,2,3,8,15...|(1000,[0,2,3,8,15...|[0.03666491072366...|0.029899439| 0.06101618|\n",
      "|10016|Delta Air Lines| 5-Jun-14|      USA|     0| Economy|    1|         NO|On May 22 after a...| 15|   5-Jun-1|[may, 6hour, dela...|(1000,[0,1,8,12,2...|(1000,[0,1,8,12,2...|[0.01631399380880...|0.032925878| 0.01582328|\n",
      "|10017|Delta Air Lines| 3-Jun-14|   Canada|     9| Economy|    4|        YES|Considering how D...| 16|   3-Jun-1|[considering, del...|(1000,[0,2,3,8,18...|(1000,[0,2,3,8,18...|[0.02790413141312...|0.028120724|0.053239778|\n",
      "|10018|Delta Air Lines| 2-Jun-14|      USA|     9| Economy|    5|        YES|Travelled MSP-LHR...| 17|   2-Jun-1|[travelled, msplh...|(1000,[0,1,3,7,8,...|(1000,[0,1,3,7,8,...|[0.08871766001172...| 0.03230644| 0.04258778|\n",
      "|10019|Delta Air Lines|29-May-14|      USA|     7|Business|    2|        YES|JFK-LAX on a 757-...| 18|   29-May-|[jfklax, 757200, ...|(1000,[0,2,7,9,11...|(1000,[0,2,7,9,11...|[0.02730338935691...|0.015876738|  0.1449797|\n",
      "|10020|Delta Air Lines|28-May-14|       UK|     9| Economy|    3|        YES|Third long haul f...| 19|   28-May-|[third, long, hau...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[0.03560480177879...|0.026255924| 0.14741424|\n",
      "+-----+---------------+---------+---------+------+--------+-----+-----------+--------------------+---+----------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Breakout LDA Topics for Modeling and Reporting\n",
    "\n",
    "def breakout_array(index_number, record):\n",
    "    vectorlist = record.tolist()\n",
    "    return vectorlist[index_number]\n",
    "\n",
    "udf_breakout_array = udf(breakout_array, FloatType())\n",
    "enrichedData = ldaResults                                                                   \\\n",
    "        .withColumn(\"Topic_12\", udf_breakout_array(lit(12), ldaResults.topicDistribution))  \\\n",
    "        .withColumn(\"topic_20\", udf_breakout_array(lit(20), ldaResults.topicDistribution))            \n",
    "\n",
    "enrichedData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, airline: string, date: string, rating: string, topic_20: float]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Register Table for SparkSQL\n",
    "enrichedData.createOrReplaceTempView(\"enrichedData\")\n",
    "\n",
    "sqlContext.sql(\"SELECT id, airline, date, rating, topic_12 FROM enrichedData\")\n",
    "\n",
    "sqlContext.sql(\"SELECT id, airline, date, rating, topic_20 FROM enrichedData\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda\n",
    "\n",
    "- https://community.hortonworks.com/articles/84781/spark-text-analytics-uncovering-data-driven-topics.html\n",
    "\n",
    "- https://stackoverflow.com/questions/41386557/unable-to-use-structfield-with-pyspark\n",
    "    \n",
    "- https://stackoverflow.com/questions/42051184/latent-dirichlet-allocation-lda-in-spark/42200429#42200429\n",
    "    \n",
    "- https://github.com/zaratsian/Spark/blob/master/text_analytics_datadriven_topics.py\n",
    "    \n",
    "- http://nbviewer.jupyter.org/gist/mizvol/eb24770ac3d5d598463f972e2a669f03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
